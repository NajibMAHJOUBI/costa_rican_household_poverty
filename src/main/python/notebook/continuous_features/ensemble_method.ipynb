{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named numpy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0839b5ea881a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named numpy"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import ADASYN, SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../../../../data/train/train.csv\")\n",
    "test = pd.read_csv(\"../../../../../data/test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../../resources/continuousFeatures\", \"r\")\n",
    "continuous_columns = f.read().split(\",\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../../resources/categoricalFeatures\", \"r\")\n",
    "categorical_columns = f.read().split(\",\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../../resources/yesNoFeaturesNames\", \"r\")\n",
    "yes_no_columns = f.read().split(\",\")\n",
    "f.close()\n",
    "\n",
    "print(\"Number of numerical columns: {0}\".format(len(continuous_columns)))\n",
    "print(\"Number of categorical columns: {0}\".format(len(categorical_columns)))\n",
    "print(\"Number of yes/no columns: {0}\".format(len(yes_no_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category, category_ount = np.unique(train[\"Target\"], return_counts=True)\n",
    "print(\"(Category, Count): {0}\".format(zip(category, category_ount)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fil Yes/No values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes=1 and no=0\n",
    "def fill_yes_no(value):\n",
    "    if value == \"yes\":\n",
    "        return 1.0\n",
    "    elif value == \"no\":\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in yes_no_columns:\n",
    "    train[\"new_{0}\".format(column)] = train.apply(lambda row: fill_yes_no(row[column]), axis=1)\n",
    "    test[\"new_{0}\".format(column)] = test.apply(lambda row: fill_yes_no(row[column]), axis=1)\n",
    "\n",
    "train = train.drop(yes_no_columns, axis=1)\n",
    "test = test.drop(yes_no_columns, axis=1)\n",
    "\n",
    "columns={\"new_{0}\".format(column): column for column in yes_no_columns}\n",
    "train = train.rename(index=str, columns=columns)\n",
    "test = test.rename(index=str, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = train.columns[train.isnull().any()].tolist() ## continuous variables\n",
    "null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"Target\",\"v2a1\", 'v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mean = train[null_columns].mean()\n",
    "# label_mean.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in null_columns:\n",
    "    train[\"new_{0}\".format(column)] = train.apply(\n",
    "    lambda row: label_mean[column] if np.isnan(row[column]) else row[column],\n",
    "    axis=1)\n",
    "    \n",
    "    test[\"new_{0}\".format(column)] = test.apply(\n",
    "    lambda row: label_mean[column] if np.isnan(row[column]) else row[column],\n",
    "    axis=1)    \n",
    "\n",
    "train = train.drop(null_columns, axis=1)\n",
    "test = test.drop(null_columns, axis=1)\n",
    "train = train.rename(index=str, columns={\"new_{0}\".format(column): column for column in null_columns})\n",
    "test = test.rename(index=str, columns={\"new_{0}\".format(column): column for column in null_columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "estimator = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "model = estimator.fit(train[continuous_columns])\n",
    "X_scaled = model.transform(train[continuous_columns])\n",
    "\n",
    "x_test_scaled = model.transform(test[continuous_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X_scaled, train[\"Target\"], test_size=0.2, stratify=train[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_validation.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category, category_count = np.unique(y_train, return_counts=True)\n",
    "print(\"(Category, Count): {0}\".format(zip(category, category_count)))\n",
    "      \n",
    "category, category_count = np.unique(y_validation, return_counts=True)\n",
    "print(\"(Category, Count): {0}\".format(zip(category, category_count)))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_prediction(reg_param, X_train, y_train, X_test):\n",
    "    estimator = LogisticRegression(penalty=\"l2\", C=reg_param)\n",
    "    model = estimator.fit(X_train, y_train)\n",
    "    return model.predict(X_train), model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_score(y_true, y_prediction):\n",
    "    accuracy = accuracy_score(y_true, y_prediction)\n",
    "    precision = precision_score(y_true, y_prediction, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_prediction, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_prediction, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(accuracy, precision, recall, f1):\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train, prediction_validation = logistic_regression_prediction(1e7, X_train, y_train, X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = all_score(y_validation, prediction_validation)\n",
    "print_score(accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mc = confusion_matrix(y_validation, prediction_validation)\n",
    "print confusion_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a dataframe\n",
    "df_cm = pd.DataFrame(confusion_mc,\n",
    "                     index = [i for i in range(0,4)],\n",
    "                     columns = [i for i in range(0,4)])\n",
    "# plot graph\n",
    "plt.figure(figsize=(6,6)) # define graph\n",
    "sns.heatmap(df_cm, annot=True) # draw heatmap, add annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(prediction_validation, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 1', 'class 2', 'class 3', 'class 4']\n",
    "print(classification_report(prediction_validation, y_validation, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced datasets - ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = ADASYN()\n",
    "X_ada, y_ada = ada.fit_sample(X_scaled, train[\"Target\"])\n",
    "\n",
    "print(X_ada.shape, y_ada.shape)\n",
    "\n",
    "X_ada_train, X_ada_validation, y_ada_train, y_ada_validation = train_test_split(X_ada, y_ada, test_size=0.2, stratify=y_ada)\n",
    "\n",
    "prediction_ada_train, prediction_ada_validation = logistic_regression_prediction(1e7, X_ada_train, y_ada_train, X_ada_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = all_score(y_ada_validation, prediction_ada_validation)\n",
    "print_score(accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mc = confusion_matrix(y_ada_validation, prediction_ada_validation)\n",
    "# print confusion_mc\n",
    "\n",
    "# convert to a dataframe\n",
    "df_cm = pd.DataFrame(confusion_mc,\n",
    "                     index = [i for i in range(0,4)],\n",
    "                     columns = [i for i in range(0,4)])\n",
    "# plot graph\n",
    "plt.figure(figsize=(6,6)) # define graph\n",
    "sns.heatmap(df_cm, annot=True) # draw heatmap, add annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced datasets - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smo = SMOTE(kind=\"svm\")\n",
    "X_resampled, y_resampled = smo.fit_sample(X_scaled, train[\"Target\"])\n",
    "\n",
    "X_smo_train, X_smo_validation, y_smo_train, y_smo_validation = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled)\n",
    "\n",
    "prediction_smo_train, prediction_smo_validation = logistic_regression_prediction(1e7, X_smo_train, y_smo_train, X_smo_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = all_score(y_smo_validation, prediction_smo_validation)\n",
    "print_score(accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mc = confusion_matrix(y_smo_validation, prediction_smo_validation)\n",
    "# print confusion_mc\n",
    "\n",
    "# convert to a dataframe\n",
    "df_cm = pd.DataFrame(confusion_mc,\n",
    "                     index = [i for i in range(0,4)],\n",
    "                     columns = [i for i in range(0,4)])\n",
    "# plot graph\n",
    "plt.figure(figsize=(6,6)) # define graph\n",
    "sns.heatmap(df_cm, annot=True) # draw heatmap, add annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 1', 'class 2', 'class 3', 'class 4']\n",
    "print(classification_report(prediction_smo_validation, y_smo_validation, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params = [1e-7, 0.000001, 0.00001, 0.0001, 0.001,0.01,0.1,1,10,100,1000]\n",
    "accuracy_scores = {\"train\": [], \"validation\": []}\n",
    "precision_scores = {\"train\": [], \"validation\": []}\n",
    "recall_scores = {\"train\": [], \"validation\": []}\n",
    "f1_scores = {\"train\": [], \"validation\": []}\n",
    "print(\"Number of neighbor: \"),\n",
    "for reg_param in reg_params:\n",
    "    print(\", {0}\".format(reg_param)),\n",
    "    prediction_smo_train, prediction_smo_validation = logistic_regression_prediction(reg_param, X_smo_train, y_smo_train, X_smo_validation)\n",
    "    accuracy, precision, recall, f1 = all_score(y_smo_train, prediction_smo_train)\n",
    "    accuracy_scores[\"train\"].append(accuracy)\n",
    "    precision_scores[\"train\"].append(precision)\n",
    "    recall_scores[\"train\"].append(recall)\n",
    "    f1_scores[\"train\"].append(f1)  \n",
    "    \n",
    "    accuracy, precision, recall, f1 = all_score(y_smo_validation, prediction_smo_validation)\n",
    "    accuracy_scores[\"validation\"].append(accuracy)\n",
    "    precision_scores[\"validation\"].append(precision)\n",
    "    recall_scores[\"validation\"].append(recall)    \n",
    "    f1_scores[\"validation\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [1/reg for reg in reg_params]\n",
    "regs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax[0][0].semilogx(regs, accuracy_scores[\"train\"], c=\"r\", label=\"train\")\n",
    "ax[0][0].semilogx(regs, accuracy_scores[\"validation\"], c=\"g\", label=\"validation\")\n",
    "ax[0][0].set_title(\"Accuracy\")\n",
    "ax[0][0].legend()\n",
    "\n",
    "ax[0][1].semilogx(regs, f1_scores[\"train\"], c=\"r\", label=\"train\")\n",
    "ax[0][1].semilogx(regs, f1_scores[\"validation\"], c=\"g\", label=\"validation\")\n",
    "ax[0][1].set_title(\"F1\")\n",
    "ax[0][1].legend()\n",
    "\n",
    "ax[1][0].semilogx(regs, precision_scores[\"train\"], c=\"r\", label=\"train\")\n",
    "ax[1][0].semilogx(regs, precision_scores[\"validation\"], c=\"g\", label=\"validation\")\n",
    "ax[1][0].set_title(\"precision\")\n",
    "ax[1][0].legend()\n",
    "\n",
    "ax[1][1].semilogx(regs, recall_scores[\"train\"], c=\"r\", label=\"train\")\n",
    "ax[1][1].semilogx(regs, recall_scores[\"validation\"], c=\"g\", label=\"validation\")\n",
    "ax[1][1].set_title(\"recall\")\n",
    "ax[1][1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = LogisticRegression(C=1000)\n",
    "model = nearest_neighbors.fit(X_smo_train, y_smo_train)\n",
    "prediction_smo_validation = model.predict(X_smo_validation)\n",
    "\n",
    "\n",
    "# print confusion_mc\n",
    "\n",
    "# convert to a dataframe\n",
    "df_cm = pd.DataFrame(confusion_mc,\n",
    "                     index = [i for i in range(0,4)],\n",
    "                     columns = [i for i in range(0,4)])\n",
    "# plot graph\n",
    "plt.figure(figsize=(6,6)) # define graph\n",
    "sns.heatmap(df_cm, annot=True) # draw heatmap, add annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 1', 'class 2', 'class 3', 'class 4']\n",
    "print(classification_report(prediction_smo_validation, y_smo_validation, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_kaggle = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(prediction_kaggle, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_kaggle = test[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"Id\": id_kaggle, \"Target\": prediction_kaggle}\n",
    "data = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"/home/ubuntu/Documents/costa_rican_household_poverty/submission/sklearn/smote/logisticRegression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
